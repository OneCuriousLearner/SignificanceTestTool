#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¨¡å‹å¯¹æ¯”ç»Ÿè®¡åˆ†æå·¥å…·
æ”¯æŒå¤šä¸ªæ¨¡å‹ä¹‹é—´çš„æ€»åˆ†å·®è·æ˜¾è‘—æ€§æ£€éªŒ
"""

import pandas as pd
import numpy as np
from scipy import stats
from scipy.stats import wilcoxon, ttest_rel, mannwhitneyu
import matplotlib.pyplot as plt
import seaborn as sns
from typing import List, Dict, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class ModelComparisonTool:
    """
    æ¨¡å‹å¯¹æ¯”ç»Ÿè®¡åˆ†æå·¥å…·ç±»
    """
    
    def __init__(self, csv_file_path: str, encoding: str = 'utf-8'):
        """
        åˆå§‹åŒ–å·¥å…·
        
        Args:
            csv_file_path: CSVæ–‡ä»¶è·¯å¾„
            encoding: æ–‡ä»¶ç¼–ç ï¼Œé»˜è®¤ä¸ºutf-8
        """
        self.csv_file_path = csv_file_path
        self.encoding = encoding
        self.df = None
        self.score_columns = []
        self.model_names = []
        
    def load_data(self) -> pd.DataFrame:
        """
        åŠ è½½CSVæ•°æ®
        
        Returns:
            pd.DataFrame: åŠ è½½çš„æ•°æ®æ¡†
        """
        try:
            self.df = pd.read_csv(self.csv_file_path, encoding=self.encoding)
            print(f"âœ… æˆåŠŸåŠ è½½æ•°æ®ï¼Œå…± {len(self.df)} è¡Œï¼Œ{len(self.df.columns)} åˆ—")
            return self.df
        except Exception as e:
            print(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return None
    
    def detect_score_columns(self, pattern: str = "åˆ†_") -> List[str]:
        """
        è‡ªåŠ¨æ£€æµ‹åˆ†æ•°å­—æ®µ
        
        Args:
            pattern: åˆ†æ•°å­—æ®µçš„å‘½åæ¨¡å¼ï¼Œé»˜è®¤ä¸º"åˆ†_"
            
        Returns:
            List[str]: åˆ†æ•°å­—æ®µåˆ—è¡¨
        """
        if self.df is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return []
        
        # æŸ¥æ‰¾åŒ…å«åˆ†æ•°å­—æ®µçš„åˆ—
        score_columns = [col for col in self.df.columns if pattern in col]
        self.score_columns = score_columns
        self.model_names = [col.replace(pattern, "") for col in score_columns]
        
        print(f"ğŸ” æ£€æµ‹åˆ° {len(score_columns)} ä¸ªåˆ†æ•°å­—æ®µ:")
        for i, (col, name) in enumerate(zip(score_columns, self.model_names)):
            print(f"  {i+1}. {col} -> {name}")
        
        return score_columns
    
    def set_score_columns(self, score_columns: List[str], model_names: Optional[List[str]] = None):
        """
        æ‰‹åŠ¨è®¾ç½®åˆ†æ•°å­—æ®µ
        
        Args:
            score_columns: åˆ†æ•°å­—æ®µåˆ—è¡¨
            model_names: æ¨¡å‹åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™ä»å­—æ®µåè‡ªåŠ¨æå–
        """
        if self.df is None:
            print("âŒ è¯·å…ˆåŠ è½½æ•°æ®")
            return
        
        # éªŒè¯å­—æ®µæ˜¯å¦å­˜åœ¨
        missing_cols = [col for col in score_columns if col not in self.df.columns]
        if missing_cols:
            print(f"âŒ ä»¥ä¸‹å­—æ®µä¸å­˜åœ¨: {missing_cols}")
            return
        
        self.score_columns = score_columns
        if model_names is None:
            self.model_names = score_columns
        else:
            self.model_names = model_names
        
        print(f"âœ… è®¾ç½®å®Œæˆï¼Œå…± {len(score_columns)} ä¸ªæ¨¡å‹")
    
    def clean_score_data(self) -> pd.DataFrame:
        """
        æ¸…ç†åˆ†æ•°å­—æ®ï¼Œå¤„ç†ç¼ºå¤±å€¼å’Œå¼‚å¸¸å€¼
        
        Returns:
            pd.DataFrame: æ¸…ç†åçš„æ•°æ®
        """
        if not self.score_columns:
            print("âŒ è¯·å…ˆè®¾ç½®åˆ†æ•°å­—æ®µ")
            return None
        
        # åˆ›å»ºåˆ†æ•°å­—æ®çš„å‰¯æœ¬
        score_df = self.df[self.score_columns].copy()
        
        # è½¬æ¢æ•°æ®ç±»å‹
        for col in self.score_columns:
            score_df[col] = pd.to_numeric(score_df[col], errors='coerce')
        
        # ç»Ÿè®¡ç¼ºå¤±å€¼
        missing_count = score_df.isnull().sum()
        if missing_count.sum() > 0:
            print("âš ï¸  å‘ç°ç¼ºå¤±å€¼:")
            for col, count in missing_count.items():
                if count > 0:
                    print(f"  {col}: {count} ä¸ªç¼ºå¤±å€¼")
        
        # ç§»é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
        original_len = len(score_df)
        score_df = score_df.dropna()
        removed_len = original_len - len(score_df)
        
        if removed_len > 0:
            print(f"ğŸ§¹ ç§»é™¤äº† {removed_len} è¡ŒåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®")
        
        print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆï¼Œå‰©ä½™ {len(score_df)} è¡Œæœ‰æ•ˆæ•°æ®")
        return score_df
    
    def calculate_basic_stats(self, score_df: pd.DataFrame) -> pd.DataFrame:
        """
        è®¡ç®—åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            score_df: åˆ†æ•°å­—æ®æ¡†
            
        Returns:
            pd.DataFrame: ç»Ÿè®¡ä¿¡æ¯
        """
        stats_data = []
        
        for i, col in enumerate(self.score_columns):
            scores = score_df[col].dropna()
            model_name = self.model_names[i] if i < len(self.model_names) else col
            stats_data.append({
                'æ¨¡å‹': model_name,
                'æ ·æœ¬æ•°': len(scores),
                'å‡å€¼': scores.mean(),
                'æ ‡å‡†å·®': scores.std(),
                'ä¸­ä½æ•°': scores.median(),
                'æœ€å°å€¼': scores.min(),
                'æœ€å¤§å€¼': scores.max(),
                '25%åˆ†ä½æ•°': scores.quantile(0.25),
                '75%åˆ†ä½æ•°': scores.quantile(0.75)
            })
        
        stats_df = pd.DataFrame(stats_data)
        return stats_df
    
    def pairwise_comparison(self, score_df: pd.DataFrame, 
                          test_type: str = 'wilcoxon',
                          alpha: float = 0.05) -> pd.DataFrame:
        """
        è¿›è¡Œä¸¤ä¸¤æ¨¡å‹å¯¹æ¯”
        
        Args:
            score_df: åˆ†æ•°å­—æ®æ¡†
            test_type: ç»Ÿè®¡æ£€éªŒç±»å‹ ('wilcoxon', 'ttest', 'mannwhitney')
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            
        Returns:
            pd.DataFrame: å¯¹æ¯”ç»“æœ
        """
        if len(self.score_columns) < 2:
            print("âŒ è‡³å°‘éœ€è¦2ä¸ªæ¨¡å‹è¿›è¡Œå¯¹æ¯”")
            return None
        
        results = []
        
        for i in range(len(self.score_columns)):
            for j in range(i + 1, len(self.score_columns)):
                model1_col = self.score_columns[i]
                model2_col = self.score_columns[j]
                model1_name = self.model_names[i] if i < len(self.model_names) else model1_col
                model2_name = self.model_names[j] if j < len(self.model_names) else model2_col
                
                scores1 = score_df[model1_col].dropna()
                scores2 = score_df[model2_col].dropna()
                
                # ç¡®ä¿ä¸¤ä¸ªæ¨¡å‹çš„æ•°æ®é•¿åº¦ä¸€è‡´
                min_len = min(len(scores1), len(scores2))
                if min_len == 0:
                    continue
                
                scores1 = scores1.iloc[:min_len]
                scores2 = scores2.iloc[:min_len]
                
                # è®¡ç®—å·®å¼‚
                diff = scores1 - scores2
                mean_diff = diff.mean()
                
                # è¿›è¡Œç»Ÿè®¡æ£€éªŒ
                if test_type == 'wilcoxon':
                    try:
                        stat, p_value = wilcoxon(scores1, scores2, alternative='two-sided')
                        test_name = "Wilcoxonç¬¦å·ç§©æ£€éªŒ"
                    except ValueError:
                        stat, p_value = np.nan, np.nan
                        test_name = "Wilcoxonç¬¦å·ç§©æ£€éªŒ(æ— æ³•è®¡ç®—)"
                elif test_type == 'ttest':
                    try:
                        stat, p_value = ttest_rel(scores1, scores2)
                        test_name = "é…å¯¹tæ£€éªŒ"
                    except ValueError:
                        stat, p_value = np.nan, np.nan
                        test_name = "é…å¯¹tæ£€éªŒ(æ— æ³•è®¡ç®—)"
                elif test_type == 'mannwhitney':
                    try:
                        stat, p_value = mannwhitneyu(scores1, scores2, alternative='two-sided')
                        test_name = "Mann-Whitney Uæ£€éªŒ"
                    except ValueError:
                        stat, p_value = np.nan, np.nan
                        test_name = "Mann-Whitney Uæ£€éªŒ(æ— æ³•è®¡ç®—)"
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„æ£€éªŒç±»å‹: {test_type}")
                    continue
                
                # åˆ¤æ–­æ˜¾è‘—æ€§
                is_significant = p_value < alpha if not np.isnan(p_value) else False
                
                results.append({
                    'æ¨¡å‹1': model1_name,
                    'æ¨¡å‹2': model2_name,
                    'æ¨¡å‹1å‡å€¼': scores1.mean(),
                    'æ¨¡å‹2å‡å€¼': scores2.mean(),
                    'å‡å€¼å·®å¼‚': mean_diff,
                    'æ£€éªŒç»Ÿè®¡é‡': stat,
                    'på€¼': p_value,
                    'æ˜¾è‘—æ€§æ°´å¹³': alpha,
                    'æ˜¯å¦æ˜¾è‘—': is_significant,
                    'æ£€éªŒæ–¹æ³•': test_name
                })
        
        results_df = pd.DataFrame(results)
        return results_df
    
    def baseline_comparison(self, score_df: pd.DataFrame, 
                           baseline_model: str,
                           test_type: str = 'wilcoxon',
                           alpha: float = 0.05) -> pd.DataFrame:
        """
        ä¸åŸºçº¿æ¨¡å‹å¯¹æ¯”
        
        Args:
            score_df: åˆ†æ•°å­—æ®æ¡†
            baseline_model: åŸºçº¿æ¨¡å‹åç§°
            test_type: ç»Ÿè®¡æ£€éªŒç±»å‹
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            
        Returns:
            pd.DataFrame: å¯¹æ¯”ç»“æœ
        """
        if baseline_model not in self.score_columns:
            print(f"âŒ åŸºçº¿æ¨¡å‹ {baseline_model} ä¸å­˜åœ¨")
            return None
        
        results = []
        baseline_scores = score_df[baseline_model].dropna()
        
        for i, model in enumerate(self.score_columns):
            if model == baseline_model:
                continue
            
            model_scores = score_df[model].dropna()
            model_name = self.model_names[i] if i < len(self.model_names) else model
            
            # ç¡®ä¿æ•°æ®é•¿åº¦ä¸€è‡´
            min_len = min(len(baseline_scores), len(model_scores))
            if min_len == 0:
                continue
            
            baseline_subset = baseline_scores.iloc[:min_len]
            model_subset = model_scores.iloc[:min_len]
            
            # è®¡ç®—å·®å¼‚
            diff = model_subset - baseline_subset
            mean_diff = diff.mean()
            
            # è¿›è¡Œç»Ÿè®¡æ£€éªŒ
            if test_type == 'wilcoxon':
                try:
                    stat, p_value = wilcoxon(model_subset, baseline_subset, alternative='two-sided')
                    test_name = "Wilcoxonç¬¦å·ç§©æ£€éªŒ"
                except ValueError:
                    stat, p_value = np.nan, np.nan
                    test_name = "Wilcoxonç¬¦å·ç§©æ£€éªŒ(æ— æ³•è®¡ç®—)"
            elif test_type == 'ttest':
                try:
                    stat, p_value = ttest_rel(model_subset, baseline_subset)
                    test_name = "é…å¯¹tæ£€éªŒ"
                except ValueError:
                    stat, p_value = np.nan, np.nan
                    test_name = "é…å¯¹tæ£€éªŒ(æ— æ³•è®¡ç®—)"
            else:
                print(f"âŒ ä¸æ”¯æŒçš„æ£€éªŒç±»å‹: {test_type}")
                continue
            
            # åˆ¤æ–­æ˜¾è‘—æ€§
            is_significant = p_value < alpha if not np.isnan(p_value) else False
            
            # åˆ¤æ–­æ¨¡å‹æ˜¯å¦ä¼˜äºåŸºçº¿
            better_than_baseline = mean_diff > 0 and is_significant
            
            results.append({
                'æ¨¡å‹': model_name,
                'åŸºçº¿æ¨¡å‹': baseline_model,
                'æ¨¡å‹å‡å€¼': model_subset.mean(),
                'åŸºçº¿å‡å€¼': baseline_subset.mean(),
                'å‡å€¼å·®å¼‚': mean_diff,
                'æ£€éªŒç»Ÿè®¡é‡': stat,
                'på€¼': p_value,
                'æ˜¾è‘—æ€§æ°´å¹³': alpha,
                'æ˜¯å¦æ˜¾è‘—': is_significant,
                'ä¼˜äºåŸºçº¿': better_than_baseline,
                'æ£€éªŒæ–¹æ³•': test_name
            })
        
        results_df = pd.DataFrame(results)
        return results_df
    
    def create_visualization(self, score_df: pd.DataFrame, 
                           save_path: Optional[str] = None) -> None:
        """
        åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        
        Args:
            score_df: åˆ†æ•°å­—æ®æ¡†
            save_path: ä¿å­˜è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™æ˜¾ç¤ºå›¾è¡¨
        """
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ¨¡å‹å¯¹æ¯”åˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. ç®±çº¿å›¾
        ax1 = axes[0, 0]
        score_df.boxplot(ax=ax1, rot=45)
        ax1.set_title('æ¨¡å‹å¾—åˆ†åˆ†å¸ƒç®±çº¿å›¾')
        ax1.set_ylabel('å¾—åˆ†')
        
        # 2. å¯†åº¦å›¾
        ax2 = axes[0, 1]
        for col in self.score_columns:
            scores = score_df[col].dropna()
            ax2.hist(scores, alpha=0.6, label=col, bins=20)
        ax2.set_title('æ¨¡å‹å¾—åˆ†åˆ†å¸ƒç›´æ–¹å›¾')
        ax2.set_xlabel('å¾—åˆ†')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.legend()
        
        # 3. å‡å€¼å¯¹æ¯”æ¡å½¢å›¾
        ax3 = axes[1, 0]
        means = [score_df[col].mean() for col in self.score_columns]
        bars = ax3.bar(range(len(self.score_columns)), means)
        ax3.set_title('æ¨¡å‹å¹³å‡å¾—åˆ†å¯¹æ¯”')
        ax3.set_xlabel('æ¨¡å‹')
        ax3.set_ylabel('å¹³å‡å¾—åˆ†')
        ax3.set_xticks(range(len(self.score_columns)))
        ax3.set_xticklabels(self.score_columns, rotation=45)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, bar in enumerate(bars):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.3f}', ha='center', va='bottom')
        
        # 4. ç›¸å…³æ€§çƒ­åŠ›å›¾
        ax4 = axes[1, 1]
        corr_matrix = score_df.corr()
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, ax=ax4)
        ax4.set_title('æ¨¡å‹å¾—åˆ†ç›¸å…³æ€§çƒ­åŠ›å›¾')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.show()
    
    def generate_report(self, score_df: pd.DataFrame, 
                       baseline_model: Optional[str] = None,
                       test_type: str = 'wilcoxon',
                       alpha: float = 0.05) -> str:
        """
        ç”Ÿæˆåˆ†ææŠ¥å‘Š
        
        Args:
            score_df: åˆ†æ•°å­—æ®æ¡†
            baseline_model: åŸºçº¿æ¨¡å‹åç§°
            test_type: ç»Ÿè®¡æ£€éªŒç±»å‹
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            
        Returns:
            str: åˆ†ææŠ¥å‘Š
        """
        report = []
        report.append("=" * 60)
        report.append("æ¨¡å‹å¯¹æ¯”ç»Ÿè®¡åˆ†ææŠ¥å‘Š")
        report.append("=" * 60)
        
        # åŸºæœ¬ä¿¡æ¯
        report.append(f"\nğŸ“Š æ•°æ®æ¦‚è§ˆ:")
        report.append(f"  - æ ·æœ¬æ•°é‡: {len(score_df)}")
        report.append(f"  - æ¨¡å‹æ•°é‡: {len(self.score_columns)}")
        report.append(f"  - ç»Ÿè®¡æ£€éªŒ: {test_type}")
        report.append(f"  - æ˜¾è‘—æ€§æ°´å¹³: Î± = {alpha}")
        
        # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        stats_df = self.calculate_basic_stats(score_df)
        report.append(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
        report.append(stats_df.to_string(index=False))
        
        # ä¸¤ä¸¤å¯¹æ¯”ç»“æœ
        pairwise_results = self.pairwise_comparison(score_df, test_type, alpha)
        report.append(f"\nğŸ” ä¸¤ä¸¤æ¨¡å‹å¯¹æ¯”ç»“æœ:")
        report.append(pairwise_results.to_string(index=False))
        
        # åŸºçº¿å¯¹æ¯”ç»“æœ
        if baseline_model:
            baseline_results = self.baseline_comparison(score_df, baseline_model, test_type, alpha)
            report.append(f"\nğŸ¯ ä¸åŸºçº¿æ¨¡å‹ {baseline_model} çš„å¯¹æ¯”ç»“æœ:")
            report.append(baseline_results.to_string(index=False))
        
        # æ€»ç»“
        report.append(f"\nğŸ“ åˆ†ææ€»ç»“:")
        significant_pairs = pairwise_results[pairwise_results['æ˜¯å¦æ˜¾è‘—'] == True]
        if len(significant_pairs) > 0:
            report.append(f"  - å‘ç° {len(significant_pairs)} å¯¹æ¨¡å‹ä¹‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚")
            for _, row in significant_pairs.iterrows():
                report.append(f"    * {row['æ¨¡å‹1']} vs {row['æ¨¡å‹2']}: p = {row['på€¼']:.4f}")
        else:
            report.append("  - æœªå‘ç°æ¨¡å‹é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚")
        
        return "\n".join(report)


def main():
    """
    ä¸»å‡½æ•° - ä½¿ç”¨ç¤ºä¾‹
    """
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸš€ æ¨¡å‹å¯¹æ¯”ç»Ÿè®¡åˆ†æå·¥å…·")
    print("=" * 50)
    
    # åˆå§‹åŒ–å·¥å…·
    tool = ModelComparisonTool("å½’å› ç»„åˆ.csv")
    
    # åŠ è½½æ•°æ®
    df = tool.load_data()
    if df is None:
        return
    
    # è‡ªåŠ¨æ£€æµ‹åˆ†æ•°å­—æ®µ
    score_columns = tool.detect_score_columns()
    
    # æ¸…ç†æ•°æ®
    score_df = tool.clean_score_data()
    if score_df is None:
        return
    
    # ç”Ÿæˆåˆ†ææŠ¥å‘Š
    report = tool.generate_report(score_df, test_type='wilcoxon', alpha=0.05)
    print(report)
    
    # åˆ›å»ºå¯è§†åŒ–
    tool.create_visualization(score_df)


if __name__ == "__main__":
    main()
